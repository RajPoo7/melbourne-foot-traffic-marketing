{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454e74a8",
   "metadata": {},
   "source": [
    "# 02 — Clean & Geo‑Join (Sensors → SA2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbfc75-825b-4153-8c9a-5189c4c421a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TODO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb63020c-bcd7-4dd3-8989-95c258a034a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /Users/poojithraj/Documents/melbourne-foot-traffic-marketing/notebooks\n",
      "notebook file is in: /Users/poojithraj/Documents/melbourne-foot-traffic-marketing/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"notebook file is in:\", pathlib.Path(\".\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2206299c-5f65-4be7-b529-7650c670e1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in: /Users/poojithraj/Documents/melbourne-foot-traffic-marketing/data/raw\n",
      "com_counts_2025_03.csv 5.04 MB\n",
      "com_counts_2025_04.csv 4.11 MB\n",
      "com_counts_2025_05.csv 4.94 MB\n",
      "sensor_locations.geojson 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "raw = Path(\"../data/raw\")   # <-- go up one level, then into data/raw\n",
    "print(\"Looking in:\", raw.resolve())\n",
    "assert raw.exists(), \"raw folder not found—path is wrong\"\n",
    "for p in sorted(raw.glob(\"*\")):\n",
    "    print(p.name, f\"{p.stat().st_size/1e6:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b4c948-c3fe-43e4-8405-d9aef52d1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# because this notebook is inside /notebooks, the project root is one level up\n",
    "BASE = Path(\"..\").resolve()\n",
    "RAW = BASE / \"data\" / \"raw\"\n",
    "INTERIM = BASE / \"data\" / \"interim\"\n",
    "INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MEL_TZ = \"Australia/Melbourne\"\n",
    "\n",
    "def std_col(name: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"_\", name.strip().lower()).strip(\"_\")\n",
    "\n",
    "def month_bounds_from_filename(p: Path):\n",
    "    m = re.search(r\"(\\d{4})[_-](\\d{2})\", p.stem)\n",
    "    if not m: return None, None\n",
    "    y, mth = int(m.group(1)), int(m.group(2))\n",
    "    start = pd.Timestamp(year=y, month=mth, day=1, tz=MEL_TZ)\n",
    "    end   = (start + pd.offsets.MonthEnd(1)).replace(hour=23)\n",
    "    return start, end\n",
    "\n",
    "def filter_to_its_month(df: pd.DataFrame, src_file: Path) -> pd.DataFrame:\n",
    "    start, end = month_bounds_from_filename(src_file)\n",
    "    if start is None: return df\n",
    "    mask = (df[\"date_time\"] >= start) & (df[\"date_time\"] <= end)\n",
    "    return df.loc[mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6667fa7f-6719-4d26-9613-9e8f7fad7d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 64,040 | Duplicates removed: 0\n",
      "Date range: 2025-03-01 00:00:00+11:00 → 2025-03-31 23:00:00+11:00\n",
      "Sensors: 96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>hourly_counts</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107</td>\n",
       "      <td>2025-03-01 18:00:00+11:00</td>\n",
       "      <td>237</td>\n",
       "      <td>com_counts_2025_03.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>2025-03-01 14:00:00+11:00</td>\n",
       "      <td>602</td>\n",
       "      <td>com_counts_2025_03.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>107</td>\n",
       "      <td>2025-03-15 02:00:00+11:00</td>\n",
       "      <td>22</td>\n",
       "      <td>com_counts_2025_03.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sensor_id                 date_time  hourly_counts             source_file\n",
       "0       107 2025-03-01 18:00:00+11:00            237  com_counts_2025_03.csv\n",
       "1        20 2025-03-01 14:00:00+11:00            602  com_counts_2025_03.csv\n",
       "2       107 2025-03-15 02:00:00+11:00             22  com_counts_2025_03.csv"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coerce_counts_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    original_cols = df.columns.tolist()\n",
    "    df = df.rename(columns={c: std_col(c) for c in df.columns})\n",
    "\n",
    "    # sensor id\n",
    "    sid = None\n",
    "    for c in (\"sensor_id\",\"location_id\",\"sensorid\",\"locationid\"):\n",
    "        if c in df.columns: sid = c; break\n",
    "    assert sid, f\"Could not find sensor id. Saw: {original_cols}\"\n",
    "\n",
    "    # counts\n",
    "    cnt = None\n",
    "    for c in (\"hourly_counts\",\"count\",\"total_of_directions\",\"total\"):\n",
    "        if c in df.columns: cnt = c; break\n",
    "    assert cnt, f\"Could not find counts column. Saw: {original_cols}\"\n",
    "\n",
    "    # timestamp (either a single column or date+hour)\n",
    "    tscol = None\n",
    "    for c in (\"date_time\",\"datetime\",\"datehour\"):\n",
    "        if c in df.columns: tscol = c; break\n",
    "\n",
    "    if tscol:\n",
    "        s = pd.to_datetime(df[tscol], errors=\"coerce\")\n",
    "        if s.dt.tz is None:\n",
    "            s = s.dt.tz_localize(MEL_TZ, ambiguous=\"NaT\", nonexistent=\"NaT\")\n",
    "        else:\n",
    "            s = s.dt.tz_convert(MEL_TZ)\n",
    "        df[\"date_time\"] = s\n",
    "    else:\n",
    "        dcol = next((c for c in (\"sensing_date\",\"date\",\"day\") if c in df.columns), None)\n",
    "        hcol = next((c for c in (\"hourday\",\"hour\",\"hr\") if c in df.columns), None)\n",
    "        assert dcol and hcol, f\"Need date + hour. Saw: {original_cols}\"\n",
    "        dd = pd.to_datetime(df[dcol], errors=\"coerce\")\n",
    "        hh = pd.to_numeric(df[hcol], errors=\"coerce\").astype(\"Int64\")\n",
    "        s  = dd + pd.to_timedelta(hh.astype(float), unit=\"h\")\n",
    "        s  = s.dt.tz_localize(MEL_TZ, ambiguous=\"NaT\", nonexistent=\"NaT\")\n",
    "        df[\"date_time\"] = s\n",
    "\n",
    "    df[\"sensor_id\"] = df[sid].astype(str).str.strip()\n",
    "    df[\"hourly_counts\"] = pd.to_numeric(df[cnt], errors=\"coerce\")\n",
    "    return df[[\"sensor_id\",\"date_time\",\"hourly_counts\"]].copy()\n",
    "\n",
    "# load & combine\n",
    "csv_paths = sorted(RAW.glob(\"com_counts_*.csv\"))\n",
    "assert csv_paths, f\"No files found in {RAW}\"\n",
    "frames = []\n",
    "for p in csv_paths:\n",
    "    t = pd.read_csv(p)\n",
    "    t = coerce_counts_schema(t)\n",
    "    t = t.dropna(subset=[\"sensor_id\",\"date_time\",\"hourly_counts\"])\n",
    "    t = filter_to_its_month(t, p)     # removes stray June/July rows from Apr/May files\n",
    "    t[\"source_file\"] = p.name\n",
    "    frames.append(t)\n",
    "\n",
    "counts = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# de-dupe & validate\n",
    "before = len(counts)\n",
    "counts = counts.drop_duplicates(subset=[\"sensor_id\",\"date_time\"])\n",
    "dups = before - len(counts)\n",
    "neg  = (counts[\"hourly_counts\"] < 0).sum()\n",
    "assert neg == 0, f\"Negative counts found: {neg}\"\n",
    "assert counts[\"date_time\"].isna().sum() == 0, \"Unparseable timestamps.\"\n",
    "\n",
    "print(f\"Rows: {len(counts):,} | Duplicates removed: {dups:,}\")\n",
    "print(\"Date range:\", counts['date_time'].min(), \"→\", counts['date_time'].max())\n",
    "print(\"Sensors:\", counts['sensor_id'].nunique())\n",
    "counts.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25338863-b014-4c49-b6a6-0a16ef957206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensors table shape: (5, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>sensor_name</th>\n",
       "      <th>sensor_description</th>\n",
       "      <th>installation_date</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181</td>\n",
       "      <td>Eli368_T</td>\n",
       "      <td>368 Elizabeth Street</td>\n",
       "      <td>2025-03-26</td>\n",
       "      <td>-37.810095</td>\n",
       "      <td>144.961431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184</td>\n",
       "      <td>Eli124_T</td>\n",
       "      <td>124 Elizabeth Street</td>\n",
       "      <td>2025-06-28</td>\n",
       "      <td>-37.815124</td>\n",
       "      <td>144.963720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185</td>\n",
       "      <td>Eli197_T</td>\n",
       "      <td>197 Elizabeth Street</td>\n",
       "      <td>2025-06-28</td>\n",
       "      <td>-37.813746</td>\n",
       "      <td>144.962762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sensor_id sensor_name    sensor_description installation_date   latitude  \\\n",
       "0       181    Eli368_T  368 Elizabeth Street        2025-03-26 -37.810095   \n",
       "1       184    Eli124_T  124 Elizabeth Street        2025-06-28 -37.815124   \n",
       "3       185    Eli197_T  197 Elizabeth Street        2025-06-28 -37.813746   \n",
       "\n",
       "    longitude  \n",
       "0  144.961431  \n",
       "1  144.963720  \n",
       "3  144.962762  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_geo = RAW / \"sensor_locations.geojson\"\n",
    "assert loc_geo.exists(), f\"{loc_geo} not found\"\n",
    "\n",
    "sensors = geopandas_read = gpd.read_file(loc_geo)\n",
    "sensors = sensors.rename(columns={c: std_col(c) for c in sensors.columns})\n",
    "\n",
    "required = {\"location_id\",\"sensor_name\",\"sensor_description\",\"installation_date\"}\n",
    "missing = required - set(sensors.columns)\n",
    "assert not missing, f\"Missing fields: {missing}. Present: {list(sensors.columns)}\"\n",
    "\n",
    "latcol = \"latitude\" if \"latitude\" in sensors.columns else \"lat\"\n",
    "loncol = \"longitude\" if \"longitude\" in sensors.columns else \"lon\"\n",
    "assert latcol in sensors.columns and loncol in sensors.columns, \"latitude/longitude not found.\"\n",
    "\n",
    "sensors_clean = sensors.copy()\n",
    "sensors_clean[\"sensor_id\"]  = sensors_clean[\"location_id\"].astype(str).str.strip()\n",
    "sensors_clean[\"latitude\"]   = pd.to_numeric(sensors_clean[latcol], errors=\"coerce\")\n",
    "sensors_clean[\"longitude\"]  = pd.to_numeric(sensors_clean[loncol], errors=\"coerce\")\n",
    "assert sensors_clean[\"latitude\"].notna().all() and sensors_clean[\"longitude\"].notna().all(), \"Some sensors missing coords.\"\n",
    "\n",
    "sensors_clean = sensors_clean[[\"sensor_id\",\"sensor_name\",\"sensor_description\",\"installation_date\",\"latitude\",\"longitude\"]]\n",
    "sensors_clean = sensors_clean.drop_duplicates(subset=[\"sensor_id\"])\n",
    "\n",
    "print(\"Sensors table shape:\", sensors_clean.shape)\n",
    "sensors_clean.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d16d612-8f61-4dec-a9b0-593cef72ad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor ID coverage: 2.1% (missing 94)\n",
      "Example missing IDs: ['1', '10', '107', '108', '109', '11', '117', '118', '12', '123']\n",
      "Wrote: /Users/poojithraj/Documents/melbourne-foot-traffic-marketing/data/interim/traffic_by_hour.csv\n",
      "Wrote: /Users/poojithraj/Documents/melbourne-foot-traffic-marketing/data/interim/sensor_locations_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# coverage\n",
    "cnt_ids = set(counts[\"sensor_id\"].unique())\n",
    "loc_ids = set(sensors_clean[\"sensor_id\"].unique())\n",
    "missing = cnt_ids - loc_ids\n",
    "coverage = 100 * (1 - len(missing)/max(1,len(cnt_ids)))\n",
    "print(f\"Sensor ID coverage: {coverage:.1f}% (missing {len(missing)})\")\n",
    "if missing:\n",
    "    print(\"Example missing IDs:\", list(sorted(missing))[:10])\n",
    "\n",
    "# save\n",
    "counts_out  = INTERIM / \"traffic_by_hour.csv\"\n",
    "sensors_out = INTERIM / \"sensor_locations_clean.csv\"\n",
    "counts[[\"sensor_id\",\"date_time\",\"hourly_counts\"]].sort_values([\"sensor_id\",\"date_time\"]).to_csv(counts_out, index=False)\n",
    "sensors_clean.to_csv(sensors_out, index=False)\n",
    "print(\"Wrote:\", counts_out)\n",
    "print(\"Wrote:\", sensors_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad54a37a-73e9-4208-8313-27e91161f499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True | Size MB: 0.05\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "p = Path(\"../data/raw/sensor_locations.geojson\")\n",
    "print(\"Exists:\", p.exists(), \"| Size MB:\", round(p.stat().st_size/1e6, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b85f2c6-de91-46c9-929d-b530a42e96bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True | Size MB: 0.05\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "p = Path(\"../data/raw/sensor_locations.geojson\")\n",
    "print(\"Exists:\", p.exists(), \"| Size MB:\", round(p.stat().st_size/1e6, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb1fde41-84fb-4a31-9107-82d0e0f3959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensors table shape: (135, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>sensor_name</th>\n",
       "      <th>sensor_description</th>\n",
       "      <th>installation_date</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bou292_T</td>\n",
       "      <td>Bourke Street Mall (North)</td>\n",
       "      <td>2009-03-24</td>\n",
       "      <td>-37.813494</td>\n",
       "      <td>144.965153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Swa123_T</td>\n",
       "      <td>Town Hall (West)</td>\n",
       "      <td>2009-03-23</td>\n",
       "      <td>-37.814880</td>\n",
       "      <td>144.966088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>BouHbr_T</td>\n",
       "      <td>Victoria Point</td>\n",
       "      <td>2009-04-23</td>\n",
       "      <td>-37.818765</td>\n",
       "      <td>144.947105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sensor_id sensor_name          sensor_description installation_date  \\\n",
       "0         1    Bou292_T  Bourke Street Mall (North)        2009-03-24   \n",
       "1         4    Swa123_T            Town Hall (West)        2009-03-23   \n",
       "2        10    BouHbr_T              Victoria Point        2009-04-23   \n",
       "\n",
       "    latitude   longitude  \n",
       "0 -37.813494  144.965153  \n",
       "1 -37.814880  144.966088  \n",
       "2 -37.818765  144.947105  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd, pandas as pd, re\n",
    "from pathlib import Path\n",
    "\n",
    "loc_geo = Path(\"../data/raw/sensor_locations.geojson\")\n",
    "sensors = gpd.read_file(loc_geo).rename(columns=lambda c: re.sub(r\"[^a-z0-9]+\",\"_\", c.lower()).strip(\"_\"))\n",
    "\n",
    "required = {\"location_id\",\"sensor_name\",\"sensor_description\",\"installation_date\"}\n",
    "missing = required - set(sensors.columns)\n",
    "assert not missing, f\"Missing fields: {missing}. Present: {list(sensors.columns)}\"\n",
    "\n",
    "latcol = \"latitude\" if \"latitude\" in sensors.columns else \"lat\"\n",
    "loncol = \"longitude\" if \"longitude\" in sensors.columns else \"lon\"\n",
    "assert latcol in sensors.columns and loncol in sensors.columns, \"latitude/longitude not found.\"\n",
    "\n",
    "sensors_clean = sensors.copy()\n",
    "sensors_clean[\"sensor_id\"]  = sensors_clean[\"location_id\"].astype(str).str.strip()\n",
    "sensors_clean[\"latitude\"]   = pd.to_numeric(sensors_clean[latcol], errors=\"coerce\")\n",
    "sensors_clean[\"longitude\"]  = pd.to_numeric(sensors_clean[loncol], errors=\"coerce\")\n",
    "assert sensors_clean[\"latitude\"].notna().all() and sensors_clean[\"longitude\"].notna().all()\n",
    "\n",
    "sensors_clean = sensors_clean[[\"sensor_id\",\"sensor_name\",\"sensor_description\",\"installation_date\",\"latitude\",\"longitude\"]]\n",
    "sensors_clean = sensors_clean.drop_duplicates(subset=[\"sensor_id\"])\n",
    "\n",
    "print(\"Sensors table shape:\", sensors_clean.shape)\n",
    "sensors_clean.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5e12fb1-277a-4fc1-a9f6-02520b4d97ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor ID coverage: 100.0% (missing 0)\n"
     ]
    }
   ],
   "source": [
    "cnt_ids = set(counts[\"sensor_id\"].unique())\n",
    "loc_ids = set(sensors_clean[\"sensor_id\"].unique())\n",
    "missing = cnt_ids - loc_ids\n",
    "coverage = 100 * (1 - len(missing)/max(1,len(cnt_ids)))\n",
    "print(f\"Sensor ID coverage: {coverage:.1f}% (missing {len(missing)})\")\n",
    "if missing:\n",
    "    print(\"Example missing IDs:\", list(sorted(missing))[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee972e7a-3818-4c33-810c-8255f32501a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ../data/interim/traffic_by_hour.csv\n",
      "Wrote: ../data/interim/sensor_locations_clean.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "INTERIM = Path(\"..\")/\"data\"/\"interim\"\n",
    "INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "counts[[\"sensor_id\",\"date_time\",\"hourly_counts\"]].sort_values([\"sensor_id\",\"date_time\"]).to_csv(INTERIM/\"traffic_by_hour.csv\", index=False)\n",
    "sensors_clean.to_csv(INTERIM/\"sensor_locations_clean.csv\", index=False)\n",
    "print(\"Wrote:\", INTERIM/\"traffic_by_hour.csv\")\n",
    "print(\"Wrote:\", INTERIM/\"sensor_locations_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2165cd0-18ca-45cd-8b61-9de5bec49536",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (970244286.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgit add data/interim/*.csv notebooks/02_clean_join_geo.ipynb\u001b[39m\n                                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "git add data/interim/*.csv notebooks/02_clean_join_geo.ipynb\n",
    "git commit -m \"Use full sensor locations; coverage ok; save clean tables\"\n",
    "git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba258f5-30f8-4387-87f2-5a3810ed7250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add/rm <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mdeleted:    notebooks/01_download_coM.ipynb\u001b[m\n",
      "\t\u001b[31mmodified:   notebooks/02_clean_join_geo.ipynb\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mMiniforge3-MacOSX-arm64.sh\u001b[m\n",
      "\t\u001b[31mnotebooks/01_download_coM_FIXED.ipynb\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "origin\thttps://github.com/RajPoo7/melbourne-foot-traffic-marketing.git (fetch)\n",
      "origin\thttps://github.com/RajPoo7/melbourne-foot-traffic-marketing.git (push)\n",
      "* \u001b[32mmain\u001b[m\n",
      "fatal: pathspec 'analytics/looker_studio_datasources' did not match any files\n",
      "[main 8dd3a7a] Clean CoM hourly counts + full sensor locations (coverage 100%); save interim outputs\n",
      " 1 file changed, 657 insertions(+), 1 deletion(-)\n",
      "Enumerating objects: 7, done.\n",
      "Counting objects: 100% (7/7), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 5.19 KiB | 5.19 MiB/s, done.\n",
      "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/RajPoo7/melbourne-foot-traffic-marketing.git\n",
      "   98f610e..8dd3a7a  main -> main\n",
      "branch 'main' set up to track 'origin/main'.\n"
     ]
    }
   ],
   "source": [
    "# sanity checks (prints repo status, remote and branch)\n",
    "!git -C .. rev-parse --is-inside-work-tree\n",
    "!git -C .. status\n",
    "!git -C .. remote -v\n",
    "!git -C .. branch\n",
    "\n",
    "# stage the notebook you edited (DO NOT add data/interim)\n",
    "!git -C .. add notebooks/02_clean_join_geo.ipynb\n",
    "\n",
    "# if you already created KPI CSVs earlier, stage that folder too (safe even if empty)\n",
    "!git -C .. add -A analytics/looker_studio_datasources\n",
    "\n",
    "# commit\n",
    "!git -C .. commit -m \"Clean CoM hourly counts + full sensor locations (coverage 100%); save interim outputs\"\n",
    "\n",
    "# push to GitHub (main branch)\n",
    "!git -C .. push -u origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aeb7581-92eb-4457-8eb9-7e1b65ff0354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ../analytics/looker_studio_datasources/heatmap.csv\n",
      "Wrote: ../analytics/looker_studio_datasources/power_hours.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load clean hourly data\n",
    "df = pd.read_csv(\"../data/interim/traffic_by_hour.csv\", parse_dates=[\"date_time\"])\n",
    "df[\"weekday\"] = df[\"date_time\"].dt.day_name()\n",
    "df[\"hour\"]    = df[\"date_time\"].dt.hour\n",
    "\n",
    "# 1) Weekday × hour heatmap\n",
    "weekday_order = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "heatmap = (df.groupby([\"weekday\",\"hour\"])[\"hourly_counts\"]\n",
    "             .mean()\n",
    "             .reset_index()\n",
    "             .rename(columns={\"hourly_counts\":\"avg_count\"}))\n",
    "heatmap[\"weekday\"] = pd.Categorical(heatmap[\"weekday\"], categories=weekday_order, ordered=True)\n",
    "heatmap = heatmap.sort_values([\"weekday\",\"hour\"])\n",
    "\n",
    "# 2) Top 5 \"power hours\" per sensor\n",
    "power = (df.groupby([\"sensor_id\",\"hour\"])[\"hourly_counts\"]\n",
    "           .mean()\n",
    "           .reset_index()\n",
    "           .rename(columns={\"hourly_counts\":\"avg_count\"}))\n",
    "power[\"rank_in_sensor\"] = power.groupby(\"sensor_id\")[\"avg_count\"].rank(method=\"first\", ascending=False)\n",
    "power_hours = power.query(\"rank_in_sensor <= 5\").sort_values([\"sensor_id\",\"rank_in_sensor\"])\n",
    "\n",
    "# Save to a git-tracked folder\n",
    "outdir = Path(\"../analytics/looker_studio_datasources\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "heatmap.to_csv(outdir/\"heatmap.csv\", index=False)\n",
    "power_hours.to_csv(outdir/\"power_hours.csv\", index=False)\n",
    "print(\"Wrote:\", outdir/\"heatmap.csv\")\n",
    "print(\"Wrote:\", outdir/\"power_hours.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96b4e978-8f55-4b83-b809-d72be4a58134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: analytics/looker_studio_datasources/*.csv\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add/rm <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mdeleted:    notebooks/01_download_coM.ipynb\u001b[m\n",
      "\t\u001b[31mmodified:   notebooks/02_clean_join_geo.ipynb\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mMiniforge3-MacOSX-arm64.sh\u001b[m\n",
      "\t\u001b[31manalytics/\u001b[m\n",
      "\t\u001b[31mnotebooks/01_download_coM_FIXED.ipynb\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# stage the two CSVs and your notebook\n",
    "!git -C .. add analytics/looker_studio_datasources/*.csv notebooks/02_clean_join_geo.ipynb\n",
    "\n",
    "# commit & push\n",
    "!git -C .. commit -m \"Export heatmap & power-hours for dashboard\"\n",
    "!git -C .. push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbbd17fb-cdce-4c6f-a8a8-3827e25fab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 1eb8475] Keep both 01_* notebooks (original + FIXED)\n",
      " 2 files changed, 178 insertions(+), 25 deletions(-)\n",
      " delete mode 100644 notebooks/01_download_coM.ipynb\n",
      " create mode 100644 notebooks/01_download_coM_FIXED.ipynb\n",
      "Enumerating objects: 6, done.\n",
      "Counting objects: 100% (6/6), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 3.37 KiB | 3.37 MiB/s, done.\n",
      "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/RajPoo7/melbourne-foot-traffic-marketing.git\n",
      "   8dd3a7a..1eb8475  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git -C .. add notebooks/01_download_coM.ipynb notebooks/01_download_coM_FIXED.ipynb\n",
    "!git -C .. commit -m \"Keep both 01_* notebooks (original + FIXED)\"\n",
    "!git -C .. push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5747a121-147e-4158-bb19-125e16592468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'notebooks/01_download_coM.ipynb' did not match any files\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   notebooks/02_clean_join_geo.ipynb\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mMiniforge3-MacOSX-arm64.sh\u001b[m\n",
      "\t\u001b[31manalytics/\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# Remove the old file from the repo, keep FIXED\n",
    "!git -C .. rm --cached notebooks/01_download_coM.ipynb  # stops tracking but leaves your local copy\n",
    "!git -C .. add notebooks/01_download_coM_FIXED.ipynb\n",
    "!git -C .. commit -m \"Replace 01_download_coM with 01_download_coM_FIXED\"\n",
    "!git -C .. push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a11aa30a-8993-4d70-bed3-9b2961a4181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ../analytics/looker_studio_datasources/heatmap.csv\n",
      "Wrote: ../analytics/looker_studio_datasources/power_hours.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load clean hourly series\n",
    "df = pd.read_csv(\"../data/interim/traffic_by_hour.csv\", parse_dates=[\"date_time\"])\n",
    "df[\"weekday\"] = df[\"date_time\"].dt.day_name()\n",
    "df[\"hour\"]    = df[\"date_time\"].dt.hour\n",
    "\n",
    "# 1) Weekday × hour heatmap\n",
    "weekday_order = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "heatmap = (df.groupby([\"weekday\",\"hour\"])[\"hourly_counts\"]\n",
    "             .mean()\n",
    "             .reset_index()\n",
    "             .rename(columns={\"hourly_counts\":\"avg_count\"}))\n",
    "heatmap[\"weekday\"] = pd.Categorical(heatmap[\"weekday\"], categories=weekday_order, ordered=True)\n",
    "heatmap = heatmap.sort_values([\"weekday\",\"hour\"])\n",
    "\n",
    "# 2) Top 5 “power hours” per sensor\n",
    "power = (df.groupby([\"sensor_id\",\"hour\"])[\"hourly_counts\"]\n",
    "           .mean()\n",
    "           .reset_index()\n",
    "           .rename(columns={\"hourly_counts\":\"avg_count\"}))\n",
    "power[\"rank_in_sensor\"] = power.groupby(\"sensor_id\")[\"avg_count\"].rank(method=\"first\", ascending=False)\n",
    "power_hours = power.query(\"rank_in_sensor <= 5\").sort_values([\"sensor_id\",\"rank_in_sensor\"])\n",
    "\n",
    "# Save (this folder is tracked by git)\n",
    "outdir = Path(\"../analytics/looker_studio_datasources\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "heatmap.to_csv(outdir/\"heatmap.csv\", index=False)\n",
    "power_hours.to_csv(outdir/\"power_hours.csv\", index=False)\n",
    "print(\"Wrote:\", outdir/\"heatmap.csv\")\n",
    "print(\"Wrote:\", outdir/\"power_hours.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4efd6834-5be6-4178-a932-affb32b60905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main c0cb15f] Export heatmap & power-hours for dashboard\n",
      " 3 files changed, 909 insertions(+), 2 deletions(-)\n",
      " create mode 100644 analytics/looker_studio_datasources/heatmap.csv\n",
      " create mode 100644 analytics/looker_studio_datasources/power_hours.csv\n",
      "Enumerating objects: 11, done.\n",
      "Counting objects: 100% (11/11), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (8/8), done.\n",
      "Writing objects: 100% (8/8), 8.73 KiB | 4.36 MiB/s, done.\n",
      "Total 8 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/RajPoo7/melbourne-foot-traffic-marketing.git\n",
      "   1eb8475..c0cb15f  main -> main\n"
     ]
    }
   ],
   "source": [
    "# Stage the two CSVs explicitly (avoids the wildcard issue)\n",
    "!git -C .. add analytics/looker_studio_datasources/heatmap.csv analytics/looker_studio_datasources/power_hours.csv\n",
    "\n",
    "# Stage the notebook you edited\n",
    "!git -C .. add notebooks/02_clean_join_geo.ipynb\n",
    "\n",
    "# Commit & push\n",
    "!git -C .. commit -m \"Export heatmap & power-hours for dashboard\"\n",
    "!git -C .. push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d763b-0c4f-424b-992a-d133f3386a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
