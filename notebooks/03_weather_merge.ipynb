{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3421509",
   "metadata": {},
   "source": [
    "# 03 — Weather Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TODO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857ca8f5-7a02-4592-b7fb-cb991d89a547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "RAW      = Path(\"../data/raw\")\n",
    "INTERIM  = Path(\"../data/interim\")\n",
    "PROCESSED= Path(\"../data/processed\")\n",
    "ANALYTICS= Path(\"../analytics/looker_studio_datasources\")\n",
    "for p in [INTERIM, PROCESSED, ANALYTICS]: p.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ca3bcc-3e41-435a-add4-46fc9b246a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts window: 2025-03-01 00:00:00+11:00 → 2025-03-31 23:00:00+11:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2025-03-01', '2025-03-31')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_path = INTERIM / \"traffic_by_hour.csv\"      # you created this in 02\n",
    "assert counts_path.exists(), f\"Missing counts: {counts_path}\"\n",
    "\n",
    "counts = pd.read_csv(counts_path, parse_dates=[\"date_time\"])\n",
    "counts = counts.sort_values(\"date_time\").reset_index(drop=True)\n",
    "\n",
    "start_ts = counts[\"date_time\"].min()\n",
    "end_ts   = counts[\"date_time\"].max()\n",
    "print(\"Counts window:\", start_ts, \"→\", end_ts)\n",
    "\n",
    "# We’ll use calendar dates for the weather API\n",
    "start_date = start_ts.date().isoformat()\n",
    "end_date   = end_ts.date().isoformat()\n",
    "start_date, end_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06511bf-c4ac-4846-ac61-38ef537715a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   temperature_2m  precipitation  rain  wind_speed_10m           date_time\n",
      "0            17.8            0.0   0.0             5.7 2025-03-01 00:00:00\n",
      "1            17.4            0.0   0.0             3.6 2025-03-01 01:00:00\n",
      "2            16.8            0.0   0.0             2.2 2025-03-01 02:00:00\n",
      "Weather rows: 744 from 2025-03-01 00:00:00 to 2025-03-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# Melbourne CBD (Flinders St / Town Hall area)\n",
    "lat, lon = -37.8136, 144.9631\n",
    "\n",
    "# Open-Meteo archive (ERA5)\n",
    "base_url = \"https://archive-api.open-meteo.com/v1/era5\"\n",
    "params = {\n",
    "    \"latitude\": lat,\n",
    "    \"longitude\": lon,\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"hourly\": \"temperature_2m,precipitation,rain,wind_speed_10m\",\n",
    "    \"timezone\": \"Australia/Melbourne\"\n",
    "}\n",
    "\n",
    "r = requests.get(base_url, params=params, timeout=60)\n",
    "r.raise_for_status()\n",
    "j = r.json()\n",
    "\n",
    "# Turn hourly arrays into a DataFrame\n",
    "weather = pd.DataFrame(j[\"hourly\"])\n",
    "# Make sure the timestamp is proper datetime\n",
    "weather[\"date_time\"] = pd.to_datetime(weather[\"time\"])   # already in local time because of timezone=\n",
    "weather = weather.drop(columns=[\"time\"])\n",
    "\n",
    "# Quick sanity check\n",
    "print(weather.head(3))\n",
    "print(\"Weather rows:\", len(weather), \"from\", weather[\"date_time\"].min(), \"to\", weather[\"date_time\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3283e0d2-360b-48d5-b494-74c19a1fb7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ../data/interim/weather_hourly_melbourne.csv | Rows: 744\n"
     ]
    }
   ],
   "source": [
    "# Ensure numeric\n",
    "for c in [\"temperature_2m\",\"precipitation\",\"rain\",\"wind_speed_10m\"]:\n",
    "    weather[c] = pd.to_numeric(weather[c], errors=\"coerce\")\n",
    "\n",
    "# A tiny helper flag for charts/filters later\n",
    "weather[\"rain_flag\"] = (weather[\"precipitation\"] > 0).astype(int)\n",
    "\n",
    "# Temperature band (nice for heatmaps)\n",
    "bins = [-100, 10, 18, 24, 30, 100]\n",
    "labels = [\"<10°C\",\"10–18°C\",\"18–24°C\",\"24–30°C\",\">30°C\"]\n",
    "weather[\"temp_band\"] = pd.cut(weather[\"temperature_2m\"], bins=bins, labels=labels)\n",
    "\n",
    "# Save a copy in interim\n",
    "weather_out = INTERIM / \"weather_hourly_melbourne.csv\"\n",
    "weather.to_csv(weather_out, index=False)\n",
    "print(\"Wrote:\", weather_out, \"| Rows:\", len(weather))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b8c262-3ee4-46b4-a023-663f90d1f2a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on datetime64[ns, UTC+11:00] and datetime64[ns] columns for key 'date_time'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Keep just the columns we need from counts (adjust names if yours differ)\u001b[39;00m\n\u001b[32m      2\u001b[39m counts_small = counts[[\u001b[33m\"\u001b[39m\u001b[33msensor_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdate_time\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mhourly_counts\u001b[39m\u001b[33m\"\u001b[39m]].copy()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m joined = \u001b[43mcounts_small\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mm:1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# each hour maps to one weather row\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mJoined rows:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(joined))\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMissing weather rows after join:\u001b[39m\u001b[33m\"\u001b[39m, joined[\u001b[33m\"\u001b[39m\u001b[33mtemperature_2m\u001b[39m\u001b[33m\"\u001b[39m].isna().sum())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/melb_foot_traffic/lib/python3.11/site-packages/pandas/core/frame.py:10839\u001b[39m, in \u001b[36mDataFrame.merge\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10820\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m  10821\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m2\u001b[39m)\n\u001b[32m  10822\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m  10835\u001b[39m     validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m ) -> DataFrame:\n\u001b[32m  10837\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10840\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10848\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10849\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10853\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/melb_foot_traffic/lib/python3.11/site-packages/pandas/core/reshape/merge.py:170\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[32m    156\u001b[39m         left_df,\n\u001b[32m    157\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         copy=copy,\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     op = \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result(copy=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/melb_foot_traffic/lib/python3.11/site-packages/pandas/core/reshape/merge.py:807\u001b[39m, in \u001b[36m_MergeOperation.__init__\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    803\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_tolerance(\u001b[38;5;28mself\u001b[39m.left_join_keys)\n\u001b[32m    805\u001b[39m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[32m    810\u001b[39m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/melb_foot_traffic/lib/python3.11/site-packages/pandas/core/reshape/merge.py:1519\u001b[39m, in \u001b[36m_MergeOperation._maybe_coerce_merge_keys\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1516\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lk.dtype, DatetimeTZDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   1517\u001b[39m     rk.dtype, DatetimeTZDtype\n\u001b[32m   1518\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1520\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lk.dtype, DatetimeTZDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   1521\u001b[39m     rk.dtype, DatetimeTZDtype\n\u001b[32m   1522\u001b[39m ):\n\u001b[32m   1523\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: You are trying to merge on datetime64[ns, UTC+11:00] and datetime64[ns] columns for key 'date_time'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# Keep just the columns we need from counts (adjust names if yours differ)\n",
    "counts_small = counts[[\"sensor_id\",\"date_time\",\"hourly_counts\"]].copy()\n",
    "\n",
    "joined = counts_small.merge(\n",
    "    weather,\n",
    "    on=\"date_time\",\n",
    "    how=\"left\",\n",
    "    validate=\"m:1\"       # each hour maps to one weather row\n",
    ")\n",
    "\n",
    "print(\"Joined rows:\", len(joined))\n",
    "print(\"Missing weather rows after join:\", joined[\"temperature_2m\"].isna().sum())\n",
    "\n",
    "# The join should keep all count rows and attach weather columns\n",
    "joined.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ef0a2d-6c0d-4dbe-bdaf-7c0f64a31423",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Australia/Melbourne' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m counts_small[\u001b[33m\"\u001b[39m\u001b[33mdate_time\u001b[39m\u001b[33m\"\u001b[39m]  = counts_small[\u001b[33m\"\u001b[39m\u001b[33mdate_time\u001b[39m\u001b[33m\"\u001b[39m].dt.floor(\u001b[33m\"\u001b[39m\u001b[33mh\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m weather[\u001b[33m\"\u001b[39m\u001b[33mdate_time\u001b[39m\u001b[33m\"\u001b[39m]       = weather[\u001b[33m\"\u001b[39m\u001b[33mdate_time\u001b[39m\u001b[33m\"\u001b[39m].dt.floor(\u001b[33m\"\u001b[39m\u001b[33mh\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcounts tz:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mcounts_small\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtz\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m())\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mweather tz:\u001b[39m\u001b[33m\"\u001b[39m, weather[\u001b[33m\"\u001b[39m\u001b[33mdate_time\u001b[39m\u001b[33m\"\u001b[39m].dt.tz.unique())\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcounts range:\u001b[39m\u001b[33m\"\u001b[39m, counts_small[\u001b[33m\"\u001b[39m\u001b[33mdate_time\u001b[39m\u001b[33m\"\u001b[39m].min(), \u001b[33m\"\u001b[39m\u001b[33m→\u001b[39m\u001b[33m\"\u001b[39m, counts_small[\u001b[33m\"\u001b[39m\u001b[33mdate_time\u001b[39m\u001b[33m\"\u001b[39m].max())\n",
      "\u001b[31mAttributeError\u001b[39m: 'Australia/Melbourne' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MEL_TZ = \"Australia/Melbourne\"\n",
    "\n",
    "def ensure_mel_tz(s):\n",
    "    \"\"\"Return a datetime Series that is tz-aware in Australia/Melbourne.\"\"\"\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    if getattr(s.dt, \"tz\", None) is None:\n",
    "        # Times are *already local wall time* from Open-Meteo; localize (don't convert)\n",
    "        return s.dt.tz_localize(MEL_TZ, ambiguous=\"infer\", nonexistent=\"shift_forward\")\n",
    "    else:\n",
    "        # If they already have a tz, convert to Melbourne to be safe\n",
    "        return s.dt.tz_convert(MEL_TZ)\n",
    "\n",
    "# 1) Standardise dtypes\n",
    "counts_small[\"date_time\"]  = ensure_mel_tz(counts_small[\"date_time\"])\n",
    "weather[\"date_time\"]       = ensure_mel_tz(weather[\"date_time\"])\n",
    "\n",
    "# 2) Align to exact hour on both sides (defensive)\n",
    "counts_small[\"date_time\"]  = counts_small[\"date_time\"].dt.floor(\"h\")\n",
    "weather[\"date_time\"]       = weather[\"date_time\"].dt.floor(\"h\")\n",
    "\n",
    "print(\"counts tz:\", counts_small[\"date_time\"].dt.tz.unique())\n",
    "print(\"weather tz:\", weather[\"date_time\"].dt.tz.unique())\n",
    "print(\"counts range:\", counts_small[\"date_time\"].min(), \"→\", counts_small[\"date_time\"].max())\n",
    "print(\"weather range:\", weather[\"date_time\"].min(), \"→\", weather[\"date_time\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "068cf993-a286-4831-bf23-d2d688cf87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MEL_TZ = \"Australia/Melbourne\"\n",
    "\n",
    "def ensure_mel_tz(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Return a datetime Series that is tz-aware in Australia/Melbourne.\"\"\"\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    if getattr(s.dt, \"tz\", None) is None:\n",
    "        # Open-Meteo times are already local wall time; attach the Melbourne tz\n",
    "        return s.dt.tz_localize(MEL_TZ, ambiguous=\"infer\", nonexistent=\"shift_forward\")\n",
    "    else:\n",
    "        # If there is a tz already, convert into Melbourne tz\n",
    "        return s.dt.tz_convert(MEL_TZ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efbc20e5-bf53-4f12-b72a-7d373bd5d4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts tz: Australia/Melbourne\n",
      "weather tz: Australia/Melbourne\n",
      "counts range: 2025-03-01 00:00:00+11:00 → 2025-03-31 23:00:00+11:00\n",
      "weather range: 2025-03-01 00:00:00+11:00 → 2025-03-31 23:00:00+11:00\n"
     ]
    }
   ],
   "source": [
    "# If you haven't already:\n",
    "# counts_small = counts[[\"sensor_id\",\"date_time\",\"hourly_counts\"]].copy()\n",
    "\n",
    "# Standardise tz\n",
    "counts_small[\"date_time\"] = ensure_mel_tz(counts_small[\"date_time\"])\n",
    "weather[\"date_time\"]      = ensure_mel_tz(weather[\"date_time\"])\n",
    "\n",
    "# Align exactly to the hour to avoid off-by-seconds\n",
    "counts_small[\"date_time\"] = counts_small[\"date_time\"].dt.floor(\"h\")\n",
    "weather[\"date_time\"]      = weather[\"date_time\"].dt.floor(\"h\")\n",
    "\n",
    "# Debug prints (no .unique() here)\n",
    "print(\"counts tz:\",  counts_small[\"date_time\"].dt.tz)\n",
    "print(\"weather tz:\", weather[\"date_time\"].dt.tz)\n",
    "print(\"counts range:\",  counts_small[\"date_time\"].min(), \"→\", counts_small[\"date_time\"].max())\n",
    "print(\"weather range:\", weather[\"date_time\"].min(),      \"→\", weather[\"date_time\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff54017-751a-4a89-bc81-c6a60587a5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined rows: 64040\n",
      "Missing weather rows after join: 0\n"
     ]
    }
   ],
   "source": [
    "wx_cols = [\"date_time\", \"temperature_2m\", \"precipitation\", \"rain\", \"wind_speed_10m\"]\n",
    "weather_small = weather[wx_cols].copy()\n",
    "\n",
    "joined = counts_small.merge(\n",
    "    weather_small,\n",
    "    on=\"date_time\",\n",
    "    how=\"left\",\n",
    "    validate=\"m:1\",   # many counts rows -> 1 weather row\n",
    ")\n",
    "\n",
    "print(\"Joined rows:\", len(joined))\n",
    "missing = joined[\"temperature_2m\"].isna().sum()\n",
    "print(\"Missing weather rows after join:\", missing)\n",
    "\n",
    "if missing:\n",
    "    # Show a few problem timestamps if any\n",
    "    display(joined.loc[joined[\"temperature_2m\"].isna(), [\"date_time\",\"sensor_id\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f3ef2b-8a17-4910-a642-1f55a9a455f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2250711377.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgit add notebooks/03_weather_merge.ipynb data/interim/weather_hourly_melbourne.csv\u001b[39m\n                        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "# from your repo root\n",
    "git add notebooks/03_weather_merge.ipynb data/interim/weather_hourly_melbourne.csv\n",
    "git commit -m \"03: weather merge — standardise tz to Australia/Melbourne and hour-align\"\n",
    "git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f365e28-e10f-45c0-8ccd-c921c5a81471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
