{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf69b54-127c-4a74-8f88-a83fb394e10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for files: /Users/poojithraj/Documents/melbourne-foot-traffic-marketing/data/raw/com_counts_2025_*.csv\n",
      "WARNING: Date range mismatch. Min Date: 2025-03-01 00:00:00, Max Date: 2025-03-31 23:00:00\n",
      "Successfully saved combined data to /Users/poojithraj/Documents/melbourne-foot-traffic-marketing/data/interim/hourly_counts_combined.csv\n",
      "SUCCESS: Loaded 11 sensor locations.\n",
      "IDs in counts but not in locations: 94\n",
      "IDs in locations but not in counts: 3\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Mismatch found! Missing location data for IDs: {np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(14), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(23), np.int64(24), np.int64(25), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(35), np.int64(36), np.int64(37), np.int64(39), np.int64(40), np.int64(41), np.int64(42), np.int64(43), np.int64(44), np.int64(45), np.int64(46), np.int64(47), np.int64(48), np.int64(49), np.int64(50), np.int64(51), np.int64(52), np.int64(53), np.int64(54), np.int64(56), np.int64(58), np.int64(59), np.int64(61), np.int64(62), np.int64(63), np.int64(66), np.int64(67), np.int64(68), np.int64(69), np.int64(70), np.int64(71), np.int64(72), np.int64(75), np.int64(76), np.int64(77), np.int64(78), np.int64(79), np.int64(84), np.int64(85), np.int64(86), np.int64(87), np.int64(107), np.int64(108), np.int64(109), np.int64(117), np.int64(118), np.int64(123), np.int64(124), np.int64(130), np.int64(131), np.int64(132), np.int64(133), np.int64(134), np.int64(135), np.int64(136), np.int64(137), np.int64(138), np.int64(139), np.int64(140), np.int64(141), np.int64(142), np.int64(143), np.int64(161), np.int64(162), np.int64(164), np.int64(165), np.int64(166), np.int64(167), np.int64(179)}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIDs in locations but not in counts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids_in_locations_not_counts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# The number of IDs that are in the counts but not locations should be 0\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids_in_counts_not_locations) == \u001b[32m0\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMismatch found! Missing location data for IDs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids_in_counts_not_locations\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Save the sensor locations to the interim folder (cleaned version)\u001b[39;00m\n\u001b[32m    109\u001b[39m gdf_sensors[[\u001b[33m'\u001b[39m\u001b[33mlocation_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msensor_name\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstreet_name\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstallation_date\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdirection_1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdirection_2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m'\u001b[39m]].to_file(\n\u001b[32m    110\u001b[39m     DATA_INTERIM_DIR / \u001b[33m\"\u001b[39m\u001b[33msensor_locations_clean.geojson\u001b[39m\u001b[33m\"\u001b[39m, driver=\u001b[33m\"\u001b[39m\u001b[33mGeoJSON\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    111\u001b[39m )\n",
      "\u001b[31mAssertionError\u001b[39m: Mismatch found! Missing location data for IDs: {np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(14), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(23), np.int64(24), np.int64(25), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(35), np.int64(36), np.int64(37), np.int64(39), np.int64(40), np.int64(41), np.int64(42), np.int64(43), np.int64(44), np.int64(45), np.int64(46), np.int64(47), np.int64(48), np.int64(49), np.int64(50), np.int64(51), np.int64(52), np.int64(53), np.int64(54), np.int64(56), np.int64(58), np.int64(59), np.int64(61), np.int64(62), np.int64(63), np.int64(66), np.int64(67), np.int64(68), np.int64(69), np.int64(70), np.int64(71), np.int64(72), np.int64(75), np.int64(76), np.int64(77), np.int64(78), np.int64(79), np.int64(84), np.int64(85), np.int64(86), np.int64(87), np.int64(107), np.int64(108), np.int64(109), np.int64(117), np.int64(118), np.int64(123), np.int64(124), np.int64(130), np.int64(131), np.int64(132), np.int64(133), np.int64(134), np.int64(135), np.int64(136), np.int64(137), np.int64(138), np.int64(139), np.int64(140), np.int64(141), np.int64(142), np.int64(143), np.int64(161), np.int64(162), np.int64(164), np.int64(165), np.int64(166), np.int64(167), np.int64(179)}"
     ]
    }
   ],
   "source": [
    "## 1. Import Packages and Setup\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "DATA_RAW_DIR = pathlib.Path.cwd().parent / \"data\" / \"raw\"\n",
    "DATA_INTERIM_DIR = pathlib.Path.cwd().parent / \"data\" / \"interim\"\n",
    "\n",
    "# Create interim directory if it doesn't exist\n",
    "DATA_INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## 2.1 Triple-Check A: Hourly Data Found?\n",
    "\n",
    "# 1. Define the search pattern for the three CSV files\n",
    "file_pattern = str(DATA_RAW_DIR / \"com_counts_2025_*.csv\")\n",
    "print(f\"Searching for files: {file_pattern}\")\n",
    "\n",
    "# 2. Get the list of file paths\n",
    "file_list = glob.glob(file_pattern)\n",
    "\n",
    "# Assert that all three files are found\n",
    "expected_files = 3\n",
    "if len(file_list) != expected_files:\n",
    "    print(f\"ERROR: Found {len(file_list)} files, expected {expected_files}.\")\n",
    "    print(\"Please check your 'data/raw' folder for missing or misnamed files.\")\n",
    "\n",
    "# 3. Read the files into one DataFrame\n",
    "all_data = []\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file)\n",
    "    all_data.append(df)\n",
    "\n",
    "df_hourly = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# 4. Data Cleaning and Transformation to match project standard structure\n",
    "# FIX 1: Rename columns to the standard format and create the Date_Time column\n",
    "df_hourly = df_hourly.rename(columns={\n",
    "    'Location_ID': 'Sensor_ID',\n",
    "    'Total_of_Directions': 'Hourly_Counts'\n",
    "})\n",
    "\n",
    "# Create the 'Date_Time' column by combining 'Sensing_Date' and 'HourDay'\n",
    "df_hourly['Date_Time'] = pd.to_datetime(\n",
    "    df_hourly['Sensing_Date'] + ' ' + df_hourly['HourDay'].astype(str).str.zfill(2) + ':00:00'\n",
    ")\n",
    "\n",
    "# C. Extract other required date/time components for the subsequent notebooks\n",
    "df_hourly['Year'] = df_hourly['Date_Time'].dt.year\n",
    "df_hourly['Month'] = df_hourly['Date_Time'].dt.month\n",
    "df_hourly['m_date'] = df_hourly['Date_Time'].dt.day\n",
    "df_hourly['Day'] = df_hourly['Date_Time'].dt.day_name()\n",
    "df_hourly['Time'] = df_hourly['Date_Time'].dt.hour\n",
    "\n",
    "# 5. Filter Data to expected date range\n",
    "# FIX 2: Filter out rogue June/July data that caused the ValueError.\n",
    "start_date = pd.Timestamp('2025-03-01 00:00:00')\n",
    "end_date = pd.Timestamp('2025-05-31 23:00:00')\n",
    "\n",
    "df_hourly = df_hourly[\n",
    "    (df_hourly['Date_Time'] >= start_date) & \n",
    "    (df_hourly['Date_Time'] <= end_date)\n",
    "].copy()\n",
    "\n",
    "# Final check of the Date Range\n",
    "min_date = df_hourly['Date_Time'].min()\n",
    "max_date = df_hourly['Date_Time'].max()\n",
    "\n",
    "# The data should cover exactly March 1 to May 31, 2025\n",
    "if min_date == start_date and max_date == end_date:\n",
    "    print(f\"SUCCESS: Data range is correct. Min Date: {min_date}, Max Date: {max_date}\")\n",
    "else:\n",
    "    print(f\"WARNING: Date range mismatch. Min Date: {min_date}, Max Date: {max_date}\")\n",
    "    \n",
    "# Save the combined, checked hourly data to interim folder\n",
    "df_hourly.to_csv(DATA_INTERIM_DIR / \"hourly_counts_combined.csv\", index=False)\n",
    "print(f\"Successfully saved combined data to {DATA_INTERIM_DIR / 'hourly_counts_combined.csv'}\")\n",
    "\n",
    "## 2.2 Triple-Check B: Sensor Locations Match?\n",
    "\n",
    "# 1. Load the sensor location GeoJSON file\n",
    "try:\n",
    "    gdf_sensors = gpd.read_file(DATA_RAW_DIR / \"sensor_locations.geojson\")\n",
    "    print(f\"SUCCESS: Loaded {len(gdf_sensors)} sensor locations.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Sensor location file not found at {DATA_RAW_DIR / 'sensor_locations.geojson'}. Please check file name and location.\")\n",
    "    assert False, \"Sensor location file not found.\"\n",
    "\n",
    "# 2. Get unique Sensor IDs from both data sources\n",
    "hourly_ids = set(df_hourly['Sensor_ID'].unique())\n",
    "location_ids = set(gdf_sensors['location_id'].unique()) # Note: The GeoJSON uses 'location_id'\n",
    "\n",
    "# 3. Check for mismatches\n",
    "# IDs that are in the hourly counts but NOT in the location file\n",
    "ids_in_counts_not_locations = hourly_ids - location_ids\n",
    "print(f\"IDs in counts but not in locations: {len(ids_in_counts_not_locations)}\")\n",
    "\n",
    "# IDs that are in the location file but NOT in the hourly counts\n",
    "ids_in_locations_not_counts = location_ids - hourly_ids\n",
    "print(f\"IDs in locations but not in counts: {len(ids_in_locations_not_counts)}\")\n",
    "\n",
    "# The number of IDs that are in the counts but not locations should be 0\n",
    "assert len(ids_in_counts_not_locations) == 0, f\"Mismatch found! Missing location data for IDs: {ids_in_counts_not_locations}\"\n",
    "\n",
    "# Save the sensor locations to the interim folder (cleaned version)\n",
    "gdf_sensors[['location_id', 'sensor_name', 'street_name', 'installation_date', 'direction_1', 'direction_2', 'status', 'latitude', 'longitude', 'geometry']].to_file(\n",
    "    DATA_INTERIM_DIR / \"sensor_locations_clean.geojson\", driver=\"GeoJSON\"\n",
    ")\n",
    "print(f\"Successfully saved cleaned sensor data to {DATA_INTERIM_DIR / 'sensor_locations_clean.geojson'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a3d33-70e8-4aeb-8772-649634d3425b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
